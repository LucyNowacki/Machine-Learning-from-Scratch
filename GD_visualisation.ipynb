{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style(theme=\"monokai\", context=\"notebook\", ticks=True, grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(sub_sample=True, add_outlier=False):\n",
    "    \"\"\"Load data and convert it to the metric system.\"\"\"\n",
    "    path_dataset = \"height_weight_genders.csv\"\n",
    "    data = np.genfromtxt(path_dataset, delimiter=\",\", skip_header=1, usecols=[1, 2])\n",
    "    gender = np.genfromtxt(path_dataset, delimiter=\",\", skip_header=1, usecols=[0],\n",
    "                           converters={0:lambda x: 0 if b\"Male\" in x else 1})\n",
    "    height = data[:, 0]\n",
    "    weight = data[:, 1]\n",
    "    # Convert to metric system\n",
    "    height *= 0.025\n",
    "    weight *= 0.454\n",
    "        # sub-sample\n",
    "    if sub_sample:\n",
    "        height = height[::50]\n",
    "        weight = weight[::50]\n",
    "    if add_outlier:\n",
    "        # outlier experiment\n",
    "        height = np.concatenate([height, [1.1, 1.2]])\n",
    "        weight = np.concatenate([weight, [51.5/0.454, 55.3/0.454]])\n",
    "        \n",
    "    return height, weight, gender\n",
    "\n",
    "def standardise(x):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    mean_x = np.mean(x, axis=0)\n",
    "    x = x - mean_x\n",
    "    std_x = np.std(x, axis=0)\n",
    "    x = x / std_x\n",
    "    return x, mean_x, std_x\n",
    "\n",
    "def build_model_data(height, weight):\n",
    "    \"\"\"Form (y,tX) to get regression data in matrix form.\"\"\"\n",
    "    y = weight\n",
    "    x = height\n",
    "    num_samples = len(y)\n",
    "    tx = np.c_[np.ones(num_samples), x]\n",
    "    return y, tx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardise(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Cost Function\n",
    "Fill in the `compute_cost` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mse(e):\n",
    "    \"\"\"Calculate the mse for vector e.\"\"\"\n",
    "    return 1/2*np.mean(e**2)\n",
    "\n",
    "\n",
    "def calculate_mae(e):\n",
    "    \"\"\"Calculate the mae for vector e.\"\"\"\n",
    "    return np.mean(np.abs(e))\n",
    "\n",
    "\n",
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the loss.\n",
    "\n",
    "    You can calculate the loss using mse or mae.\n",
    "    \"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    return calculate_mse(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(y, tx, w0, w1):\n",
    "    \"\"\"Algorithm for grid search.\"\"\"\n",
    "    loss = np.zeros((len(w0), len(w1)))\n",
    "    # compute loss for each combinationof w0 and w1.\n",
    "    for ind_row, row in enumerate(w0):\n",
    "        for ind_col, col in enumerate(w1):\n",
    "            w = np.array([row, col])\n",
    "            loss[ind_row, ind_col] = compute_loss(y, tx, w)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_w(num_intervals):\n",
    "    \"\"\"Generate a grid of values for w0 and w1.\"\"\"\n",
    "    w0 = np.linspace(-100, 200, num_intervals)\n",
    "    w1 = np.linspace(-150, 150, num_intervals)\n",
    "    return w0, w1\n",
    "\n",
    "def get_best_parameters(w0, w1, losses):\n",
    "    \"\"\"Get the best w from the result of grid search.\"\"\"\n",
    "    min_row, min_col = np.unravel_index(np.argmin(losses), losses.shape)\n",
    "    return losses[min_row, min_col], w0[min_row], w1[min_col]\n",
    "\n",
    "def prediction(w0, w1, mean_x, std_x):\n",
    "    \"\"\"Get the regression line from the model.\"\"\"\n",
    "    x = np.arange(1.2, 2, 0.01)\n",
    "    x_normalized = (x - mean_x) / std_x\n",
    "    return x, w0 + w1 * x_normalized\n",
    "\n",
    "def base_visualisation(grid_losses, w0_list, w1_list,\n",
    "                       mean_x, std_x, height, weight):\n",
    "    \"\"\"Base Visualization for both models.\"\"\"\n",
    "    w0, w1 = np.meshgrid(w0_list, w1_list)\n",
    "\n",
    "    fig = plt.figure()\n",
    "\n",
    "    # plot contourf\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    cp = ax1.contourf(w0, w1, grid_losses.T, cmap=plt.cm.jet)\n",
    "    fig.colorbar(cp, ax=ax1)\n",
    "    ax1.set_xlabel(r'$w_0$')\n",
    "    ax1.set_ylabel(r'$w_1$')\n",
    "    # put a marker at the minimum\n",
    "    loss_star, w0_star, w1_star = get_best_parameters(\n",
    "        w0_list, w1_list, grid_losses)\n",
    "    ax1.plot(w0_star, w1_star, marker='*', color='r', markersize=20)\n",
    "\n",
    "    # plot f(x)\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    ax2.scatter(height, weight, marker=\".\", color='b', s=5)\n",
    "    ax2.set_xlabel(\"x\")\n",
    "    ax2.set_ylabel(\"y\")\n",
    "    ax2.grid()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def grid_visualisation(grid_losses, w0_list, w1_list,\n",
    "                       mean_x, std_x, height, weight):\n",
    "    \"\"\"Visualize how the trained model looks like under the grid search.\"\"\"\n",
    "    fig = base_visualisation(\n",
    "        grid_losses, w0_list, w1_list, mean_x, std_x, height, weight)\n",
    "\n",
    "    loss_star, w0_star, w1_star = get_best_parameters(\n",
    "        w0_list, w1_list, grid_losses)\n",
    "    # plot prediciton\n",
    "    x, f = prediction(w0_star, w1_star, mean_x, std_x)\n",
    "    ax2 = fig.get_axes()[2]\n",
    "    ax2.plot(x, f, 'r')\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=250)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent (GD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An element is called a local miminum if $\\exists$ $\\hat{w}$ s.t. \n",
    "$E(\\hat{w}) \\leq E(w)\\,\\,\\,\\,\\forall\\, w\\,\\,\\,\\,with\\,\\,\\, ||w-\\hat{w}||\\leq\\epsilon $.\n",
    "\n",
    "An element is called a global minimum point if \n",
    "$E(\\hat{w}) \\leq E(w)\\,\\,\\,\\,\\forall\\,w\\in \\mathbb{R}^n $.\n",
    "For exmaple, for regression we conclude that\n",
    "$$\\hat{w}=(X^T X)^{-1}X^Ty$$\n",
    "satisfies\n",
    "$\\hat{w}=arg\\,\\,\\underset{w}{\\min } MSE\\,(w)$ for $MSE\\,(w)=\\frac{1}{2s}||Xw-y||^2$\n",
    "\n",
    "If MSE is convex, then\n",
    "$\\Delta MSE\\,(\\hat{w})=0$ which results in $MSE\\,(\\hat{w})\\leq MSE\\,(w)\\,\\,\\,\\,\\,\\forall\\,w\\in \\mathbb{R}^n$.\n",
    "However, in case of problems without closed-form solutions , we need to use systematic approaches such as GD.\n",
    "\n",
    "Smooth functions allow the appliction of more sysytematic search compared to the grid search\n",
    "$$E\\in C^1 (\\mathbb{R}^n)\\,\\,\\,\\,\\,\\,\\Longrightarrow \\Delta E \\,\\, exists$$\n",
    "Example of smooth optimisiation is GD:\n",
    "$$w^{k+1}=w^k-\\tau\\Delta E(w^k)$$\n",
    "for some $w_0\\in \\mathbb{R}^n$ and constant $\\tau > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GD for one-parameter MSE-model:    $\\,\\,\\,\\,\\,\\,\\text{MSE} w=\\frac{1}{2s}{\\sum _{j=1}^s \\left| w-y_j\\right| {}^2}$<br>\n",
    "Gradient:      $\\,\\,\\,\\,\\,\\Delta MSE(w)-\\frac{1}{s} \\sum _{j=1}^s y_j $<br>\n",
    "GD: $\\,\\,\\,\\,\\,\\,w^{k+1}=(1-\\tau)w^k+\\frac{\\tau}{s} \\sum _{j=1}^s y_j $\n",
    "\n",
    "#### For the general linear MSE: $\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,MSE(w)=\\frac{1}{2s}\\left| Xw-y \\right|^2$<br>\n",
    "Recall: $\\,\\,\\,\\Delta MSE(w)=\\frac{1}{s}X^T(Xw-y)$<br>\n",
    "GD: $\\,\\,\\,\\,w^{k+1}=w^k+\\frac{\\tau}{s}X^T(y-Xw^k)=\\left(I-\\frac{\\tau}{s}X^T X \\right) y+\\frac{\\tau}{s}X^T y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    err = y - tx.dot(w)\n",
    "    grad = -tx.T.dot(err) / len(err)\n",
    "    return grad, err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute loss, gradient\n",
    "        grad, err = compute_gradient(y, tx, w)\n",
    "        loss = calculate_mse(err)\n",
    "        # gradient w by descent update\n",
    "        w = w - gamma * grad\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=2792.2367127591674, w0=7.3293922002105205, w1=1.347971243498896\n",
      "Gradient Descent(1/49): loss=2264.635056030003, w0=13.925845180399996, w1=2.5611453626479035\n",
      "Gradient Descent(2/49): loss=1837.2777140793794, w0=19.862652862570513, w1=3.6530020698820147\n",
      "Gradient Descent(3/49): loss=1491.118267099375, w0=25.205779776523983, w1=4.63567310639271\n",
      "Gradient Descent(4/49): loss=1210.7291150455712, w0=30.0145939990821, w1=5.520077039252342\n",
      "Gradient Descent(5/49): loss=983.6139018819906, w0=34.3425267993844, w1=6.316040578826005\n",
      "Gradient Descent(6/49): loss=799.6505792194903, w0=38.23766631965648, w1=7.032407764442302\n",
      "Gradient Descent(7/49): loss=650.6402878628647, w0=41.74329188790136, w1=7.677138231496974\n",
      "Gradient Descent(8/49): loss=529.9419518639979, w0=44.89835489932174, w1=8.257395651846178\n",
      "Gradient Descent(9/49): loss=432.176299704916, w0=47.737911609600076, w1=8.779627330160464\n",
      "Gradient Descent(10/49): loss=352.9861214560597, w0=50.29351264885059, w1=9.24963584064332\n",
      "Gradient Descent(11/49): loss=288.842077074486, w0=52.59355358417605, w1=9.67264350007789\n",
      "Gradient Descent(12/49): loss=236.88540112541122, w0=54.66359042596896, w1=10.053350393569003\n",
      "Gradient Descent(13/49): loss=194.80049360666067, w0=56.526623583582584, w1=10.395986597711007\n",
      "Gradient Descent(14/49): loss=160.71171851647273, w0=58.20335342543484, w1=10.704359181438809\n",
      "Gradient Descent(15/49): loss=133.09981069342055, w0=59.712410283101875, w1=10.98189450679383\n",
      "Gradient Descent(16/49): loss=110.73416535674828, w0=61.070561455002206, w1=11.231676299613351\n",
      "Gradient Descent(17/49): loss=92.61799263404367, w0=62.2928975097125, w1=11.45647991315092\n",
      "Gradient Descent(18/49): loss=77.943892728653, w0=63.39299995895177, w1=11.658803165334731\n",
      "Gradient Descent(19/49): loss=66.05787180528654, w0=64.38309216326711, w1=11.84089409230016\n",
      "Gradient Descent(20/49): loss=56.430194857359716, w0=65.27417514715091, w1=12.00477592656905\n",
      "Gradient Descent(21/49): loss=48.63177652953898, w0=66.07614983264634, w1=12.152269577411047\n",
      "Gradient Descent(22/49): loss=42.315057684004195, w0=66.79792704959222, w1=12.285013863168846\n",
      "Gradient Descent(23/49): loss=37.19851541912101, w0=67.44752654484351, w1=12.404483720350866\n",
      "Gradient Descent(24/49): loss=33.054116184565615, w0=68.03216609056967, w1=12.512006591814684\n",
      "Gradient Descent(25/49): loss=29.69715280457577, w0=68.55834168172322, w1=12.60877717613212\n",
      "Gradient Descent(26/49): loss=26.978012466783984, w0=69.03189971376142, w1=12.695870702017812\n",
      "Gradient Descent(27/49): loss=24.775508793172605, w0=69.4581019425958, w1=12.774254875314936\n",
      "Gradient Descent(28/49): loss=22.99148081754737, w0=69.84168394854674, w1=12.844800631282347\n",
      "Gradient Descent(29/49): loss=21.546418157290944, w0=70.18690775390259, w1=12.908291811653017\n",
      "Gradient Descent(30/49): loss=20.375917402483257, w0=70.49760917872285, w1=12.965433873986619\n",
      "Gradient Descent(31/49): loss=19.42781179108901, w0=70.77724046106108, w1=13.016861730086863\n",
      "Gradient Descent(32/49): loss=18.659846245859693, w0=71.02890861516549, w1=13.063146800577082\n",
      "Gradient Descent(33/49): loss=18.037794154223942, w0=71.25540995385946, w1=13.104803364018277\n",
      "Gradient Descent(34/49): loss=17.53393195999899, w0=71.45926115868403, w1=13.142294271115354\n",
      "Gradient Descent(35/49): loss=17.125803582676767, w0=71.64272724302614, w1=13.176036087502723\n",
      "Gradient Descent(36/49): loss=16.795219597045776, w0=71.80784671893404, w1=13.206403722251356\n",
      "Gradient Descent(37/49): loss=16.527446568684663, w0=71.95645424725116, w1=13.233734593525124\n",
      "Gradient Descent(38/49): loss=16.310550415712164, w0=72.09020102273657, w1=13.258332377671517\n",
      "Gradient Descent(39/49): loss=16.134864531804435, w0=72.21057312067343, w1=13.28047038340327\n",
      "Gradient Descent(40/49): loss=15.992558965839178, w0=72.3189080088166, w1=13.300394588561847\n",
      "Gradient Descent(41/49): loss=15.877291457407319, w0=72.41640940814547, w1=13.318326373204567\n",
      "Gradient Descent(42/49): loss=15.783924775577509, w0=72.50416066754144, w1=13.334464979383014\n",
      "Gradient Descent(43/49): loss=15.70829776329537, w0=72.58313680099782, w1=13.348989724943618\n",
      "Gradient Descent(44/49): loss=15.647039883346832, w0=72.65421532110855, w1=13.36206199594816\n",
      "Gradient Descent(45/49): loss=15.597421000588525, w0=72.71818598920821, w1=13.373827039852248\n",
      "Gradient Descent(46/49): loss=15.557229705554294, w0=72.77575959049791, w1=13.384415579365928\n",
      "Gradient Descent(47/49): loss=15.52467475657656, w0=72.82757583165863, w1=13.39394526492824\n",
      "Gradient Descent(48/49): loss=15.498305247904602, w0=72.8742104487033, w1=13.40252198193432\n",
      "Gradient Descent(49/49): loss=15.476945945880312, w0=72.91618160404349, w1=13.410241027239794\n",
      "Gradient Descent: execution time=0.017 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "#from plots import gradient_descent_visualisation\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(w0, w1, mean_x, std_x):\n",
    "    \"\"\"Get the regression line from the model.\"\"\"\n",
    "    x = np.arange(1.2, 2, 0.01)\n",
    "    x_normalized = (x - mean_x) / std_x\n",
    "    return x, w0 + w1 * x_normalized\n",
    "\n",
    "\n",
    "def base_visualisation(grid_losses, w0_list, w1_list,\n",
    "                       mean_x, std_x, height, weight):\n",
    "    \"\"\"Base Visualization for both models.\"\"\"\n",
    "    w0, w1 = np.meshgrid(w0_list, w1_list)\n",
    "\n",
    "    fig = plt.figure()\n",
    "\n",
    "    # plot contourf\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    cp = ax1.contourf(w0, w1, grid_losses.T, cmap=plt.cm.jet)\n",
    "    fig.colorbar(cp, ax=ax1)\n",
    "    ax1.set_xlabel(r'$w_0$')\n",
    "    ax1.set_ylabel(r'$w_1$')\n",
    "    # put a marker at the minimum\n",
    "    loss_star, w0_star, w1_star = get_best_parameters(\n",
    "        w0_list, w1_list, grid_losses)\n",
    "    ax1.plot(w0_star, w1_star, marker='*', color='r', markersize=20)\n",
    "\n",
    "    # plot f(x)\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    ax2.scatter(height, weight, marker=\".\", color='b', s=5)\n",
    "    ax2.set_xlabel(\"x\")\n",
    "    ax2.set_ylabel(\"y\")\n",
    "    ax2.grid()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def grid_visualisation(grid_losses, w0_list, w1_list,\n",
    "                       mean_x, std_x, height, weight):\n",
    "    \"\"\"Visualize how the trained model looks like under the grid search.\"\"\"\n",
    "    fig = base_visualisation(\n",
    "        grid_losses, w0_list, w1_list, mean_x, std_x, height, weight)\n",
    "\n",
    "    loss_star, w0_star, w1_star = get_best_parameters(\n",
    "        w0_list, w1_list, grid_losses)\n",
    "    # plot prediciton\n",
    "    x, f = prediction(w0_star, w1_star, mean_x, std_x)\n",
    "    ax2 = fig.get_axes()[2]\n",
    "    ax2.plot(x, f, 'r')\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def gradient_descent_visualisation(\n",
    "        gradient_losses, gradient_ws,\n",
    "        grid_losses, grid_w0, grid_w1,\n",
    "        mean_x, std_x, height, weight, n_iter=None):\n",
    "    \"\"\"Visualize how the loss value changes until n_iter.\"\"\"\n",
    "    fig = base_visualisation(\n",
    "        grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "\n",
    "    ws_to_be_plotted = np.stack(gradient_ws)\n",
    "    if n_iter is not None:\n",
    "        ws_to_be_plotted = ws_to_be_plotted[:n_iter]\n",
    "\n",
    "    ax1, ax2 = fig.get_axes()[0], fig.get_axes()[2]\n",
    "    ax1.plot(\n",
    "        ws_to_be_plotted[:, 0], ws_to_be_plotted[:, 1],\n",
    "        marker='o', color='w', markersize=10)\n",
    "    pred_x, pred_y = prediction(\n",
    "        ws_to_be_plotted[-1, 0], ws_to_be_plotted[-1, 1],\n",
    "        mean_x, std_x)\n",
    "    ax2.plot(pred_x, pred_y, 'r')\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b7975f9bf341fda542246e8a28ae73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "#from plots import *\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualisation(\n",
    "        gradient_losses, gradient_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(14.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "\n",
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient from just few examples n and their corresponding y_n labels.\"\"\"\n",
    "    err = y - tx.dot(w)\n",
    "    grad = -tx.T.dot(err) / len(err)\n",
    "    return grad, err\n",
    "\n",
    "def stochastic_gradient_descent(\n",
    "        y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Stochastic gradient descent.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient and loss\n",
    "            grad, _ = compute_stoch_gradient(y_batch, tx_batch, w)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # calculate loss\n",
    "            loss = compute_loss(y, tx, w)\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "\n",
    "        print(\"SGD({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD(0/49): loss=2322.0845701589747, w0=6.759649180807213, w1=-0.18000589012225826\n",
      "SGD(1/49): loss=1851.7338489799918, w0=13.712918388271106, w1=2.398199424997337\n",
      "SGD(2/49): loss=1479.288724585339, w0=19.85654496630883, w1=4.979570732219344\n",
      "SGD(3/49): loss=1234.8621301709481, w0=24.89788142792651, w1=3.642246229370392\n",
      "SGD(4/49): loss=1033.776065517326, w0=29.412720621823546, w1=2.9335984245981757\n",
      "SGD(5/49): loss=835.2022632975414, w0=33.81867631361184, w1=4.46096924702144\n",
      "SGD(6/49): loss=649.7802622358078, w0=38.25202188052891, w1=7.0880002540918765\n",
      "SGD(7/49): loss=548.250092148382, w0=41.41576118307755, w1=6.443287820505029\n",
      "SGD(8/49): loss=456.9340450184698, w0=44.513557985625276, w1=6.077890944424077\n",
      "SGD(9/49): loss=382.9168217962344, w0=47.00631185068698, w1=6.844697711428561\n",
      "SGD(10/49): loss=307.41055565733956, w0=49.703740369814405, w1=8.230649077345982\n",
      "SGD(11/49): loss=257.56521526770416, w0=51.895971763930774, w1=8.333220386772005\n",
      "SGD(12/49): loss=207.15055952135194, w0=54.362836939051924, w1=8.46539676478171\n",
      "SGD(13/49): loss=169.5413455805195, w0=56.32349366513924, w1=8.97244303065082\n",
      "SGD(14/49): loss=138.905042602636, w0=58.05767415552007, w1=9.620300429897645\n",
      "SGD(15/49): loss=115.4029612051701, w0=59.519023439269745, w1=10.272483684763158\n",
      "SGD(16/49): loss=100.02148295122082, w0=60.591170483455414, w1=10.667010171999996\n",
      "SGD(17/49): loss=84.09009924641803, w0=61.87232535597816, w1=10.842374229836514\n",
      "SGD(18/49): loss=65.91455805005934, w0=63.32408246652987, w1=12.191442278019139\n",
      "SGD(19/49): loss=54.66436495750391, w0=64.50584570671741, w1=12.327901164080293\n",
      "SGD(20/49): loss=46.43449913812472, w0=65.45561521286119, w1=12.66843614648921\n",
      "SGD(21/49): loss=42.69412217883139, w0=65.93606375880458, w1=12.78805450544782\n",
      "SGD(22/49): loss=37.76365904976109, w0=66.65319340716027, w1=12.66960991669626\n",
      "SGD(23/49): loss=34.52980477895054, w0=67.14331397681248, w1=12.8030627919335\n",
      "SGD(24/49): loss=31.547924402270596, w0=67.62274898803062, w1=13.077382084015152\n",
      "SGD(25/49): loss=28.777072210264688, w0=68.13814037645032, w1=13.03218085908602\n",
      "SGD(26/49): loss=25.81028680235337, w0=68.74525147415821, w1=13.081724627092278\n",
      "SGD(27/49): loss=23.648531558311287, w0=69.26054536713738, w1=12.972602684087382\n",
      "SGD(28/49): loss=21.946899768283362, w0=69.70229709104666, w1=13.008273894804073\n",
      "SGD(29/49): loss=20.43676293272582, w0=70.13594535026392, w1=13.120639191460283\n",
      "SGD(30/49): loss=19.293128244032125, w0=70.53625621350817, w1=13.021716639367497\n",
      "SGD(31/49): loss=19.1500783609272, w0=70.56739813829664, w1=13.17238751852752\n",
      "SGD(32/49): loss=18.329885776608535, w0=70.88204197711569, w1=13.213572320359692\n",
      "SGD(33/49): loss=17.929927934111458, w0=71.0477053141687, w1=13.273337239290242\n",
      "SGD(34/49): loss=17.392540065943006, w0=71.31262431190937, w1=13.18346278173703\n",
      "SGD(35/49): loss=16.79752761929481, w0=71.62731279104001, w1=13.265952666736174\n",
      "SGD(36/49): loss=16.61423199452852, w0=71.74761078539956, w1=13.223568193420776\n",
      "SGD(37/49): loss=16.37420653110049, w0=71.89221372994163, w1=13.370849014638473\n",
      "SGD(38/49): loss=16.385225621422464, w0=71.88108587001376, w1=13.429021518218203\n",
      "SGD(39/49): loss=16.155593298423923, w0=72.0780568667621, w1=13.232563023000738\n",
      "SGD(40/49): loss=15.959873171779536, w0=72.22732945270816, w1=13.377972868098741\n",
      "SGD(41/49): loss=15.654603520132909, w0=72.56801097785272, w1=13.58210622130324\n",
      "SGD(42/49): loss=15.538879186900498, w0=72.74394899937306, w1=13.420447500938613\n",
      "SGD(43/49): loss=15.50507234786946, w0=72.83724658168502, w1=13.30703783215417\n",
      "SGD(44/49): loss=15.475070071406945, w0=73.0330316178395, w1=13.147597074680633\n",
      "SGD(45/49): loss=15.444664034081928, w0=72.98169947754397, w1=13.338045834614531\n",
      "SGD(46/49): loss=15.417220109147063, w0=73.05897851367365, w1=13.393306197488812\n",
      "SGD(47/49): loss=15.386341232913026, w0=73.30700139329693, w1=13.506835445391769\n",
      "SGD(48/49): loss=15.388965619815089, w0=73.33970357327412, w1=13.543427030747642\n",
      "SGD(49/49): loss=15.404913912226538, w0=73.44598006444268, w1=13.60190258382457\n",
      "SGD: execution time=0.055 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 15\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71ca1e17110447318e66e11325ea98af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualisation(\n",
    "        sgd_losses, sgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(25.0, 12.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "tor",
   "language": "python",
   "name": "tor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "state": {
    "d2b2c3aea192430e81437f33ba0b0e69": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "e4a6a7a70ccd42ddb112989c04f2ed3f": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
